% LINEAR SYSTEMS AND CONTROL
% Homework 3 : 

\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphics} 
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}

\usepackage{trfsigns}
\author{Ana Huaman}
\title{\textbf{Homework 3} \\ ECE 6550: Linear Control and Systems}
\date{}
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
Consider a particle whose acceleration is controlled directly through

\[ \ddot{y} = u \]

Let $x_{1} = y$ and $x_2 = \dot{y}$

\subsection*{a}
Find the transition matrix $\Phi(t,t_{0})$ and reachability Grammian $\Gamma$ associated with this system

\subsection*{Solution}

First, let's write down our equations:

\[ \dot{x} = 
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}
x + 
\begin{bmatrix}
0 \\ 
1
\end{bmatrix}
u
\]
\[ y = \begin{bmatrix}1 & 0\end{bmatrix}x \]

Now we are good. Let's find $\Phi(t, t_{0})$. We know that:

\[ \dot{\Phi}(t,t_{0}) = A\Phi(t,t_{0}) \]
\[ \dot{\Phi}(t,t_{0}) = 
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix} \Phi(t, t_{0}) \]
\[ \Phi(t,t_{0})  =  e^{ 
\begin{bmatrix}
0 & 1 \\ 
0 & 0 
\end{bmatrix}(t - t_{0}) } 
\]
\begin{center}
\fbox{ $\Phi(t,t_{0})  =  \begin{bmatrix} 1 & (t - t_{0}) \\ 0 & 1 \end{bmatrix}$ }
\end{center}

Okay, half of this section done. Let's finish the other one, the Grammian:

\[ \Gamma = \int_{t_{0}}^{t_{1}} \Phi(t_{1}, s)B(s)B^{T}(s)\Phi^{T}(t_{1}, s)ds \]
\[ \Gamma = \int_{t_{0}}^{t_{1}} 
\begin{bmatrix} 1 & (t - t_{0}) \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} 0 \\ 1 \end{bmatrix}
\begin{bmatrix} 0 && 1 \end{bmatrix}
\begin{bmatrix} 1 & 0 \\ (t_{1} - s) & 1 \end{bmatrix} ds \]

\[ \Gamma = \int_{t_{0}}^{t_{1}} 
\begin{bmatrix} 
(t_{1} - s)^{2} & (t_{1} - s) \\ 
(t_{1} - s) & 1 
\end{bmatrix} ds \]

\begin{center}
\fbox{ $\Gamma =
\begin{bmatrix} 
\dfrac{(t_{1} - t_{0})^{3}}{3} & \dfrac{(t_{1} - t_{0})^{2}}{2} \\ 
\dfrac{(t_{1} - t_{0})^{2}}{2} &  (t_{1} - t_{0})
\end{bmatrix}$ }
\end{center}


\subsection*{b}
Recall that the optimal $u$, i.e. the control signal that minimizes

\[ \int_{t_{0}}^{t_{1}} u^{T}(t)u(t)dt \]

while driving the system from $x(t_{0}) = x_{0}$ to $x(t_{1}) = x_{1}$ is given by

\[ u(t) = B^{T}\Phi^{T}(t_{1},t)\Gamma^{-1}(x_{1} - \Phi(t_{1}, t_{0})x_{0}) \]

as long as the system is completely controllable.

Compute the optimal control action for $\ddot{y} = u$ that takes the system from $y(0) = 0$, $\dot{y}(0) = 0$ to $y(1) = 1$, $\dot{y}(1) = 0$ 
  
\subsection*{Solution}
From the equation:

\[ u(t) = B^{T}\Phi^{T}(t_{1},t)\Gamma^{-1}(x_{1} - \Phi(t_{1}, t_{0})x_{0}) \]

We just have to replace. Let's lay out every factor:
\begin{equation}
B^{T} = \begin{bmatrix} 0 && 1 \end{bmatrix} 
\label{Eq:P1-BT}
\end{equation}
\begin{equation}
\Phi^{T}(t_{1},t)  = \begin{bmatrix} 1 & 0 \\ (1 - t) & 1 \end{bmatrix} 
\label{Eq:P1-Phi}
\end{equation}

For $\Gamma^{-1}$, since it is only a $2 \times 2$, the calculation is trivial:
\[ 
\Gamma^{-1} =
\dfrac{12}{(t_{1} - t_{0})^{4}}
\begin{bmatrix} 
(t_{1} - t_{0}) & -\dfrac{(t_{1} - t_{0})^{2}}{2} \\ 
-\dfrac{(t_{1} - t_{0})^{2}}{2} &  \dfrac{(t_{1} - t_{0})^{3}}{3}
\end{bmatrix} 
\]

Replacing $t_{1} = 1$ and $t_{0} = 0$

\begin{equation}
\Gamma^{-1} =
\begin{bmatrix} 
12 & -6 \\ 
-6 & 4
\end{bmatrix} 
\label{Eq:P1-Gamma}
\end{equation}

Finally (phew!) the last term (replacing the values):
\[x_{1} - \Phi(t_{1}, t_{0})x_{0} = 
\begin{bmatrix} 1 \\ 0 \end{bmatrix}
-
\begin{bmatrix} 
1 & 1 \\ 
0 & 1
\end{bmatrix} 
\begin{bmatrix} 0 \\ 0 \end{bmatrix}
\]

\begin{equation}
x_{1} - \Phi(t_{1}, t_{0})x_{0} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
\label{Eq:P1-Diff}
\end{equation}

Replacing all the numbered equations above we get:

\[ u(t) = 
\begin{bmatrix} 0 && 1 \end{bmatrix}
\begin{bmatrix} 1 & 0 \\ (1 - t) & 1 \end{bmatrix} 
\begin{bmatrix} 
12 & -6 \\ 
-6 & 4
\end{bmatrix} 
\begin{bmatrix} 1 \\ 0 \end{bmatrix} \]

\begin{center}
\fbox{ $u(t) = 6 - 12t$}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
Consider the scalar, nonlinear system

\[ \dot{x} = ax^{p} \]

where $a$ is a non-zero, real number and $p$ is a positive integer. For what values of $a$ and $p$ is the system  asymptotically stable? For what values is it unstable?

\subsection*{Solution}
Let's start with the cases in which the system is asymptotically stable by using Lyapunov functions. Let's start by considering a function $V$ such as:

\[ V(x) = x^{2n} \]

such that $n$ is whatever positive integer. It is easy to see that $V(x) > 0, \forall x \neq 0 $. Now, let's check $\dot{V}(x)$:

\[ \dot{V}(x) = (2n)x^{2n-1}\dot{x} \]
\[ \dot{V}(x) = (2n)x^{2n-1}ax^{p} \]
\[ \dot{V}(x) = (2na)x^{p + 2n-1} \]

For the system to be asymptotically stable, we need:

\[ \dot{V}(x) < 0, \forall x \neq 0 \]

Since $x$ can take whatever value, a good way to restrict $\dot{V}(x)$ to be negative is to set the coefficient $(2na)$ to be negative and the exponential of $x$ to be positive (so the sign keeps negative in the product).  Hence:

\[ 2na < 0 \rightarrow a < 0 \]
\[ x^{p+2n-1} > 0 \rightarrow (p+2n-1) = 2k \rightarrow p = 2m-1 \]

So, we find that for our system to be \textbf{asymptotically stable}, we require:
\begin{center}
\fbox{ $\begin{matrix} a < 0 \\
p = 2m - 1 \end{matrix}$  }
\end{center}

Or in other words, for the system to be asymptotically stable, $a$ must be negative and $p$ must be odd.
\medskip

Now, let's check \textbf{inestability} and let's use the hint of our instructor since I did not have idea how to do this. Let's analyze the signs that $\dot{x}$ has for x being positive or negative:

\[
\dot{x} = \left\{
\begin{array}{l l}
(-) & \quad \text{ $x:(-)$, $a:(+)$, $p:$ $odd$} \\
(+) & \quad \text{ $x:(-)$, $a:(+)$, $p:$ $even$} \\
(+) & \quad \text{ $x:(-)$, $a:(-)$, $p:$ $odd$} \\
(-) & \quad \text{ $x:(-)$, $a:(-)$, $p:$ $even$} \\
(+) & \quad \text{ $x:(+)$, $a:(+)$, $p:$ $odd$} \\
(+) & \quad \text{ $x:(+)$, $a:(+)$, $p:$ $even$} \\
(-) & \quad \text{ $x:(+)$, $a:(-)$, $p:$ $odd$} \\
(-) & \quad \text{ $x:(+)$, $a:(-)$, $p:$ $even$} \\
\end{array}
\right. \]

So, what can we say about this? First, the system is going to be evidently unestable in two possible cases (hope I am right with this one):

\begin{itemize}
\item{If $x = (+)$ and $\dot{x} = (+)$, then $x$ will grow and end up in $+\infty$}
\item{If $x = (-)$ and $\dot{x} = (-)$, then $x$ will decrease and end up in $-\infty$}
\end{itemize}

From our options we select the options that ALWAYS go to infinite:

\begin{itemize}
\item{If $x = (-)$, $a:(+)$, $p:odd$  and $\dot{x} = (-)$, then $x$ will go to $-\infty$}
\item{If $x = (-)$, $a:(-)$, $p:even$ and $\dot{x} = (-)$, then $x$ will go to $-\infty$}
\item{If $x = (+)$, $a:(+)$, $p:odd$  and $\dot{x} = (+)$, then $x$ will go to $+\infty$}
\item{If $x = (+)$, $a:(+)$, $p:even$ and $\dot{x} = (+)$, then $x$ will go to $+\infty$}
\end{itemize}

Now, these options are classified by $x$ being $+$ or $-$. But $x$ can start being either positive or negative. For both cases, we see that there is one combination that is \textbf{always inestable}, whatever the value of $x$:

\begin{center}
\fbox{ $\begin{matrix} a > 0 \\
p = 2m - 1 \end{matrix}$  }
\end{center}

Or in other words, the system is always inestable if $a > 0$ and $p$ is odd.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
This question concerns the \textit{duality} between controllability and observability. Show that the system

\[
\begin{matrix}
\dot{x} = Ax \\
y = Cx
\end{matrix}
\]
is completely observable if and only if the system

\[ \dot{x} = A^{T}x + C^{T}u \]

is completely controllable

\subsection*{Solution}

We are asked to demonstrate that if the system

\begin{equation} 
\dot{x} = A^{T}x + C^{T}u
\label{Eq:P3-S1} 
\end{equation}

is controllable, then the system:

\begin{equation}
\begin{matrix}
\dot{x} = Ax \\
y = Cx
\end{matrix}
\label{Eq:P3-S2}
\end{equation}

is observable. So,let's analize first the system (\ref{Eq:P3-S1}). For this LTI system to be completely controllable, we first check its Controllability matrix ($\Gamma_{C}$) :

\[ \Gamma_{C} = 
\begin{bmatrix}
C^{T} & A^{T}C^{T} & (A^{T})^{2}C^{T} & ... & (A^{T})^{n-1}C^{T} 
\end{bmatrix}
\]

Arranging properly:

\[ \Gamma_{C} = 
\begin{bmatrix}
C^{T} & A^{T}C^{T} & (A^{2})^{T}C^{T} & ... & (A^{n-1})^{T}C^{T} 
\end{bmatrix}
\]

\begin{equation} 
\Gamma_{C} = 
\begin{bmatrix}
C^{T} & (CA)^{T} & (CA^{2})^{T} & ... & (CA^{n-1})^{T} 
\end{bmatrix}
\label{Eq:P3-GC1a}
\end{equation}

So, four our system in (\ref{Eq:P3-S1}) to be \textit{completely controllable}, it must verify that:

\begin{equation} \mathcal{R} (\Gamma_{C}) = n
\label{Eq:P3-GC1b} 
\end{equation}

Considering $A_{n\times n}$, $C_{m \times n}$ and $x \in \Re^{n} $.
\medskip

Now, from Linear Algebra we know that for the range of a matrix $M$:

\[ \mathcal{R}(M) =  \mathcal{R}(M^{T}) \]

Instead of $M$, from Eq. (\ref{Eq:P3-GC1b}):

\begin{equation} 
\mathcal{R} (\Gamma_{C}) = n \rightarrow  \mathcal{R} (\Gamma^{T}_{C}) = n 
\label{Eq3:-EqGrammians}
\end{equation}

Let's write down what  $\Gamma^{T}_{C}$ actually is:

\[
\Gamma^{T}_{C} = 
\begin{bmatrix}
C^{T} & (CA)^{T} & (CA^{2})^{T} & ... & (CA^{n-1})^{T} 
\end{bmatrix}^{T}
\]

Simplifying the transposes, resulting:

\begin{equation}
\Gamma^{T}_{C} = 
\begin{bmatrix}
C \\
CA \\
CA^{2} \\
 ... \\
CA^{n-1} 
\end{bmatrix}
\label{Eq:P3-GT-Cc}
\end{equation}

Looks familiar? Yes! This actually has the same form of the \textit{Observability Matrix} ($\Gamma_{O}$) of a system defined as:

\begin{equation}
\begin{matrix}
\dot{x} = Ax \\
y = Cx
\end{matrix}
\label{Eq:P3-S2b}
\end{equation}

So, we have that $\Gamma_{O} = \Gamma^{T}_{C}$ (watch out that these are from two systems). We have already found in (\ref{Eq3:-EqGrammians}) that the rank of $\Gamma^{T}_{C}$ is $n$, hence:
 
\[ \Gamma_{O} = \Gamma^{T}_{C} \rightarrow \mathcal{R}( \Gamma_{O} ) = \mathcal{R}(\Gamma^{T}_{C}) = n \]

\begin{center}
\fbox{ $\mathcal{R}( \Gamma_{O} ) = n$ }
\end{center}

So, for the system in (\ref{Eq:P3-S2b}), with $A_{n\times n}$ matrix, $C_{m \times n}$ matrix and $x \in \Re^{n} $ we have that the Observability matrix has rank $n$. Hence, it \textit{is observable}. Since it is easy to see that the system in (\ref{Eq:P3-S2b}) is exactly the same as the system in (\ref{Eq:P3-S2}), we have just demonstrated that the Controllability of the system (\ref{Eq:P3-S1}) implies the Observability of the system in (\ref{Eq:P3-S2}). Yay!


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
Let

\[ \dot{x} = 
\begin{bmatrix}
0 & 1\\
1 - \alpha & 1
\end{bmatrix}
x + 
\begin{bmatrix}
\alpha \\
1
\end{bmatrix}
\]

For what values for $\alpha$ is this system completely controllable? Interpret your answer

% ---------------------------
\subsection*{Solution}
This system is LTI, so our life is easier. For the system to be completely controllable, let's check its \textit{Controllability matrix}:

\[ \Gamma_{C} = 
\begin{bmatrix} B & AB \end{bmatrix}
\]

Calculating quickly:

\[ B =  
\begin{bmatrix}
\alpha \\
1
\end{bmatrix}
\]

\[ AB =  
\begin{bmatrix}
0 & 1\\
1 - \alpha & 1
\end{bmatrix}
\begin{bmatrix}
\alpha \\
1
\end{bmatrix} = 
\begin{bmatrix}
1  \\
\alpha(1-\alpha) + 1
\end{bmatrix} 
\]

\[ \Gamma_{C} = 
\begin{bmatrix}
\alpha & 1 \\
1 & \alpha(1-\alpha) + 1
\end{bmatrix} 
\]

For the system to be completely controllable, $\Gamma_{C}$ must be of rank $n =2$. An easy way to check this is to calculate the determinant. If it is different of zero, then it is full rank:

\[ \det (\Gamma_{C} ) = 
\left | \begin{matrix}
\alpha & 1 \\
1 & \alpha(1-\alpha) + 1
\end{matrix} \right |
\]

\[ \det (\Gamma_{C} ) = 
(\alpha - 1)^{2}(\alpha + 1)
\]

We can see that the determinant is different of zero $ \forall \alpha \in \Re - \{-1, 1\} $, 
\medskip

So, what does this mean? This mean that for whatever value of $\alpha$ but $\{-1,1\}$, our system  is completely controllable, meaning that we can go from whatever state $x_{0}$ to whatever state $x_{1}$ without problem. For $\alpha = -1$ or $\alpha = 1$ this is not possible. For instance for the case of $\alpha = 1$ we find that $\dot{x}$ only depends on $x_{2}$ and $u$, being actually $\dot{x}_{1} = \dot{x}_{2}$, so there is no way to control the system completely. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
Show that the controllable canonical realization does in fact result in a completely controllable system
% -------------------------
\subsection*{Solution}
Hum, in my book there are two ways to demonstrate this. Both can be deducted by induction.

%........................
\subsubsection*{First method}
First, consider that we have a system in its controllable canonical form:

\[ \dot{x} = A_{C}x + B_{C}u \]

we have to demonstrate that it is \textit{Completely Controllable}. If this assertment is true, then the Controllability Matrix of this system must be full rank, this is:

\[ \dot{x} = A_{C}x + B_{C}u \]

must have rank $n$ ($\mathcal{R}(\Gamma_{C}) = n$ ), with $(A_{C})_{n \times n}$, $(B_{C})_{n \times m}$. 

Now, what are $B_{C}$ and $A_{C}$ ?

\[
A_{C} =
\begin{bmatrix}
0 & 1 & 0 & 0 & 0 & ... & 0 \\
0 & 0 & 1 & 0 & 0 & ... & 0 \\
0 & 0 & 0 & 1 & 0 & ... & 0 \\
. & . & . & . & . & ... & . \\
. & . & . & . & . & ... & . \\
. & . & . & . & . & ... & . \\\\
. & . & . & . & . & ... & . \\
0 & 0 & 0 & 0 & 0 & ... & 1 \\
-a_{0} & -a_{1} & -a_{2} & -a_{3} & -a_{4} & ... & -a_{n-1} 
\end{bmatrix}_{n \times n}
\]

\[
B_{C} =
\begin{bmatrix}
0 \\
0 \\
0 \\
. \\
. \\
1 
\end{bmatrix}_{n \times 1}
\]


Observe $B_{C}$. It is full of zeros but the last element. So, if we have a matrix $M$, then the matrix product $M \times B_{C}$ will yield the \textit{last column} of $M$. So, going back to our \textit{Controllability matrix}:

\[ \Gamma_{C} = 
\begin{bmatrix}
B_{C} & A_{C}B_{C} & A_{C}^{2}B_{C} & ... A_{C}^{n-1}B_{C}
\end{bmatrix}
\]

This matrix will be composed of the last columns of 

\[ \{ I, A_{C}, A^{2}_{C}, A^{3}_{C}...A^{n-1}_{C} \} \]

Now, we don't have to multiply the whole thing (unless you have lots of time, and that is not the case). Due to the particular form of $A_{C}$ we can deduce how the last columns of $A_{C}^{n}$ will be. Here the last columns for the first powers of $A_{C}$ (denoted as $\mathcal{LC}$). 

\[ \mathcal{LC}( A_{C}^{0} ) = 
\begin{bmatrix}
0 & 0 & 0 & 0 & ...& 0 & 0 & 1
\end{bmatrix}^{T}
\]
\[ \mathcal{LC}( A_{C}^{1} ) = 
\begin{bmatrix}
0 & 0 & 0 & 0 & ... & 0 & 1 & a_{n}
\end{bmatrix}^{T}
\]
\[ \mathcal{LC}( A_{C}^{2} ) = 
\begin{bmatrix}
0 & 0 & 0 & 0 & ... & 0 & 1 & a_{n}
\end{bmatrix}^{T}
\]
\[ \mathcal{LC}( A_{C}^{2} ) = 
\begin{bmatrix}
0 & 0 & 0 & 0 & ... & 1 & -a_{n-1} & (-a_{n-2} -a_{n-1}^{2})
\end{bmatrix}^{T}
\]

So, if we line them up together to form the Controllability matrix, we get a matrix of the form:

\[ \Gamma_{C} = 
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & ... & 1 \\
. & . & . & . & . & ... & . \\
. & . & . & 1 & . & ... & . \\
0 & 0 & 1 & c_{n-3,3} & c_{n-3,4} & ... & c_{n-3,n-1} \\
0 & 1 & c_{n-2,2} & c_{n-2,3} & c_{n-2,4} & ... & c_{n-2,n-1} \\
1 & c_{n-1,1} & c_{n-1,2} & c_{n-1,3} & c_{n-1,4} & ... & c_{n-1,n-1} 
\end{bmatrix}
\]

Our Controllability matrix ($n \times nm$) turns out to be of \textbf{triangular form}. It is straighforward to see that the \textbf{rank of the row space is equal to the number of rows $n$} (or in other words, look at the rows, they are independent since there is no way to linearly combine them and produce a zero, other than the trivial solution).

So, our \textit{Canonical Controllable form} produces a \textit{Controllability Matrix with rank $n$}. Hence, \textbf{a system in Canonical Controllable form is Completely Controllable}.

%........................
\subsubsection*{Second method}
Hum, this is more elegant, I guess. Let's say that we have a system such as:

\[ \dot{x} = Ax + Bu \]

So, if this system is Controllable, its Controllability Matrix $\Gamma_{C}$:

\[ \Gamma_{C} = 
\begin{bmatrix}
B & AB & A^{2}B & ... A^{n-1}B
\end{bmatrix}
\]

has rank $n$, meaning that its $n$ columns are independent (without loss of generality, consider that $B_{n \times 1}$ and $A_{n \times n}$).

Now, let's apply a transformation to our system. For this, we will use as basis the columns of the Controllability Matrix above (since we have just said that they are linearly independent). However, instead of using directly the columns, we will use a combination of them. We call this transformation $T$.
\medskip

We define $T$ as:

\begin{equation}
T = 
\begin{bmatrix}
A^{n-1}B & A^{n-2}B & ... & AB & B
\end{bmatrix}
\begin{bmatrix}
1 &  0 & 0 & 0  & ... & 0 \\
a_{n-1} & 1 & 0 & 0 & ... & 0 \\
a_{n-2} & a_{n-1} & 1 & 0 & ... & 0 \\
. & . & . & . &  ... & . \\
. & . & . & . &  ... & . \\
a_{1} & a_{2} & a_{3} & a_{4} & ... & 1 
\end{bmatrix}
\label{Eq:P4-T1}
\end{equation}
  
Where $a_{i}$ are the coefficients of the characteristic polynomial. Hence, for Cayley-Hamilton we know that the following is true:

\[ A^{n} + a_{n-1}A^{n-1} + a_{n-2}A^{n-2} + ... + a_{1}A + a_{0} = 0 \]  
\begin{center}
\fbox{ $A^{n} = -a_{n-1}A^{n-1} -a_{n-2}A^{n-2} + ...  - a_{1}A - a_{0}$ }
\end{center}

Keep that in mind. Now, going back to the transformation. We know that if we apply a transformation $T$ to the system $(A,B)$, the resulting system:

\[ \dot{\hat{x}} = \hat{A}\hat{x} + \hat{B}\hat{u} \]

where: 
\[ \begin{matrix}
\hat{A} = T^{-1}AT \\
\hat{B} = T^{-1}B
\end{matrix} \]

Let's find this $\hat{A}$ and $\hat{B}$, from the equations above we can deduce that:

\begin{equation} 
\hat{A} = T^{-1}AT \rightarrow T\hat{A} = AT 
\label{Eq:P4-A1}
\end{equation}
\begin{equation}
\hat{B} = T^{-1}B \rightarrow T\hat{B} = B
\label{Eq:P4-B1}
\end{equation}

We find $\hat{A}$ and $\hat{B}$ from Equations (\ref{Eq:P4-A1}) and (\ref{Eq:P4-B1}). To make a long story short, $\hat{A}$ and $\hat{B}$ are going to be the Canonical Controllable form of the system. Here I will show the proof for the case of $n = 2$ and $n = 3$. It holds for higuer values.

\subsubsection*{n = 2}

\[
T = 
\begin{bmatrix}
AB & B
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
a_{1} & 1
\end{bmatrix}
\]

\[
T = 
\begin{bmatrix}
AB + a_{1}B & B 
\end{bmatrix}
\]

Applying in (\ref{Eq:P4-A1}):

\[ T\hat{A} = AT \]
\[
\begin{bmatrix}
AB + a_{1}B & B 
\end{bmatrix}
\hat{A} =
\begin{bmatrix}
A^{2}B + a_{1}AB & AB 
\end{bmatrix}
\]

Using Cayley-Hamilton to reduce the $A^{2}$ factor in the right side of the equation:

\[
\begin{bmatrix}
AB + a_{1}B & B 
\end{bmatrix}
\hat{A} =
\begin{bmatrix}
(-a_{1}A-a_{0})B + a_{1}AB & AB 
\end{bmatrix}
\]
\[
\begin{bmatrix}
AB + a_{1}B & B 
\end{bmatrix}
\hat{A} =
\begin{bmatrix}
-a_{0}B & AB 
\end{bmatrix}
\]
From the equation above, we can find $\hat{A}$:


\begin{equation}
\hat{A} =
\begin{bmatrix}
0 & 1 \\
-a_{0} & -a_{1} 
\end{bmatrix}
\label{Eq:P4-AC2} 
\end{equation}

Now we calculate $\hat{B}$ from equation (\ref{Eq:P4-B1}):

\[ T\hat{B} = B \]
\[ \begin{bmatrix}
AB & B
\end{bmatrix}
\hat{B} = 
B \]

The solution is evident:

\begin{equation}
\hat{B} = 
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\label{Eq:P4-BC2}
\end{equation}

Note that (\ref{Eq:P4-AC2}) and (\ref{Eq:P4-BC2}) are in the form of a  \textbf{Controllability Canonical Form}. Awesome! Just for fun, let's check for $n = 3$ too:

\subsubsection*{n = 3}

\[
T = 
\begin{bmatrix}
A^{2}B & AB & B
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0\\
a_{2} & 1 & 0 \\
a_{1} & a_{2} & 1
\end{bmatrix}
\]

\[
T = 
\begin{bmatrix}
A^{2}B + a_{2}AB + a_{1}B & AB + a_{2}B & B 
\end{bmatrix}
\]

Applying in (\ref{Eq:P4-A1}):

\[ T\hat{A} = AT \]
\[
\begin{bmatrix}
A^{2}B + a_{2}AB + a_{1}B & AB + a_{2}B & B 
\end{bmatrix}
\hat{A} =
\begin{bmatrix}
A^{3}B + a_{2}A^{2}B + a_{1}AB & A^{2}B + a_{2}AB & AB 
\end{bmatrix}
\]

Using Cayley-Hamilton to reduce the $A^{3}$ factor in the right side of the equation:

\[
\begin{bmatrix}
A^{2}B + a_{2}AB + a_{1}B & AB + a_{2}B & B
\end{bmatrix}
\hat{A} =
\begin{bmatrix}
(-a_{2}A^{2}-a_{1}A-a_{0})B + a_{2}A^{2}B + a_{1}AB & A^{2}B + a_{2}AB & AB 
\end{bmatrix}
\]
\[
\begin{bmatrix}
A^{2}B + a_{2}AB + a_{1}B & AB + a_{2}B & B\end{bmatrix}
\hat{A} =
\begin{bmatrix}
-a_{0}B & A^{2}B + a_{2}AB & AB 
\end{bmatrix}
\]

From the equation above, we can find $\hat{A}$:

\begin{equation}
\hat{A} =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
-a_{0} & -a_{1} & -a_{2} 
\end{bmatrix}
\label{Eq:P4-AC3} 
\end{equation}

And now we calculate $\hat{B}$ from equation (\ref{Eq:P4-B1}):

\[ T\hat{B} = B \]
\[ \begin{bmatrix}
A^{2}B & AB & B
\end{bmatrix}
\hat{B} = 
B \]

The solution is evident:

\begin{equation}
\hat{B} = 
\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
\label{Eq:P4-BC3}
\end{equation}

Note that (\ref{Eq:P4-AC3}) and (\ref{Eq:P4-BC3}) are also in the form of a  \textbf{Controllability Canonical Form}. Awesome again!. Then, we are also verifying that, starting from a completely  controllable system, we apply a transformation $T$ (with a basis such as the combination of the rows of $\Gamma_{C}$ and we get as a result the Controllable Canonical Form, which keeps the same properties of the original system, meaning: The Controllable Canonical Form has also rank $n$ and consequently, it is \textbf{completely controllable!!} 

\end{document}